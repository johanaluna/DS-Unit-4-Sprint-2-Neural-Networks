{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "A neuron is the basic unit of a neural network. A neuron has a set of inputs, weights and an activation fuction that is responsible to translate all this information into a single output.\n",
    "\n",
    "- **Input Layer:**\n",
    "The input layer is the leayer in which I storage all the information from the outside word that is going to be computed for the network.\n",
    "\n",
    "- **Hidden Layer:**\n",
    "It's a layer of neurons between the input layer and the output layer. These layers perfom computations with a set of weighted inputs and give an output through an activation fuction.\n",
    "- **Output Layer:**\n",
    "It's the last layer and basically this layer produces the end result, taking as an input all the results  from the hidden layers , computationing them and transferring thisinformation from the network to the outside world\n",
    "- **Activation:** \n",
    "It's a function that decide whether a neuron should be activated or not after calculate the weights sum and further adding bias.\n",
    "\n",
    "- **Backpropagation:**\n",
    "Backpropagation is when a neural network propagates an input data forward through  hidden layers towards the output layer, then networkâ€™s is measured with a loss function and this error is backpropagated to adjust the wrong-headed parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 2), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 2), (2000, 2), (8000,), (2000,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, rate=0.1, niter=1000):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize Weights\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            err = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                delta_w = self.rate * (target - self.predict(xi))\n",
    "                self.weight[1:] += delta_w * xi\n",
    "                self.weight[0] += delta_w\n",
    "                err += int(delta_w != 0.0)\n",
    "            self.errors.append(err)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x7f5ecef19cf8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn = Perceptron()\n",
    "pn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4905\n"
     ]
    }
   ],
   "source": [
    "predictions = [pn.predict(X_test[i]) for i in range(len(X_test))]\n",
    "print(\"accuracy:\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[2.15424395e-98]\n",
      " [1.56875491e-75]\n",
      " [2.15424395e-98]\n",
      " ...\n",
      " [2.15424395e-98]\n",
      " [2.15424395e-98]\n",
      " [1.56875491e-75]]\n",
      "Loss: \n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "cost = []\n",
    "for i in range(1000):\n",
    "    cost.append(np.mean(np.square(y - nn.feed_forward(X))))\n",
    "    nn.train(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"Predicted Output: \\n\" + str(nn.feed_forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - nn.feed_forward(X)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  A perceptron is a single layer in neural network that works as a linear binary classifier. For that reason a perceptron just can do a feed-forward and cannot implement backpropagation which, it implement the feed-forward and after this calculate and error and propagarate this error back and adjust the weights. For that reason a multilayer has better perfomance because learn from its error and we have better results in the accuracy of our predictions ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>112</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>160</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>300</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "78    52    1   1       128   205    1        1      184      0      0.0   \n",
       "183   58    1   2       112   230    0        0      165      0      2.5   \n",
       "133   41    1   1       110   235    0        1      153      0      0.0   \n",
       "39    65    0   2       160   360    0        0      151      0      0.8   \n",
       "236   58    1   0       125   300    0        0      171      0      0.0   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "78       2   0     2       1  \n",
       "183      1   1     3       0  \n",
       "133      2   0     2       1  \n",
       "39       2   0     2       1  \n",
       "236      2   2     3       0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras \n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer, OrdinalEncoder\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_transform = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(242, 13) (61, 13) (242,) (61,)\n"
     ]
    }
   ],
   "source": [
    "# Split values into x and y components\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_transform[:, :-1], df_transform[:, -1],  test_size=0.20, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                168       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 363\n",
      "Trainable params: 363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/50\n",
      "242/242 [==============================] - 1s 5ms/sample - loss: 0.6678 - accuracy: 0.6198 - val_loss: 0.6963 - val_accuracy: 0.4590\n",
      "Epoch 2/50\n",
      "242/242 [==============================] - 0s 415us/sample - loss: 0.6786 - accuracy: 0.5537 - val_loss: 0.6918 - val_accuracy: 0.4754\n",
      "Epoch 3/50\n",
      "242/242 [==============================] - 0s 407us/sample - loss: 0.6679 - accuracy: 0.5702 - val_loss: 0.6795 - val_accuracy: 0.5574\n",
      "Epoch 4/50\n",
      "242/242 [==============================] - 0s 395us/sample - loss: 0.6677 - accuracy: 0.5909 - val_loss: 0.6723 - val_accuracy: 0.6230\n",
      "Epoch 5/50\n",
      "242/242 [==============================] - 0s 458us/sample - loss: 0.6452 - accuracy: 0.6364 - val_loss: 0.6623 - val_accuracy: 0.6557\n",
      "Epoch 6/50\n",
      "242/242 [==============================] - 0s 482us/sample - loss: 0.6515 - accuracy: 0.6405 - val_loss: 0.6547 - val_accuracy: 0.6721\n",
      "Epoch 7/50\n",
      "242/242 [==============================] - 0s 426us/sample - loss: 0.6258 - accuracy: 0.6529 - val_loss: 0.6491 - val_accuracy: 0.6885\n",
      "Epoch 8/50\n",
      "242/242 [==============================] - 0s 451us/sample - loss: 0.6187 - accuracy: 0.6694 - val_loss: 0.6450 - val_accuracy: 0.7049\n",
      "Epoch 9/50\n",
      "242/242 [==============================] - 0s 487us/sample - loss: 0.6236 - accuracy: 0.6777 - val_loss: 0.6327 - val_accuracy: 0.6557\n",
      "Epoch 10/50\n",
      "242/242 [==============================] - 0s 407us/sample - loss: 0.5938 - accuracy: 0.6777 - val_loss: 0.6194 - val_accuracy: 0.6393\n",
      "Epoch 11/50\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.6031 - accuracy: 0.6777 - val_loss: 0.6125 - val_accuracy: 0.6393\n",
      "Epoch 12/50\n",
      "242/242 [==============================] - 0s 443us/sample - loss: 0.5549 - accuracy: 0.7273 - val_loss: 0.6035 - val_accuracy: 0.6557\n",
      "Epoch 13/50\n",
      "242/242 [==============================] - 0s 407us/sample - loss: 0.5598 - accuracy: 0.7562 - val_loss: 0.6020 - val_accuracy: 0.6557\n",
      "Epoch 14/50\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.5570 - accuracy: 0.7190 - val_loss: 0.5936 - val_accuracy: 0.6557\n",
      "Epoch 15/50\n",
      "242/242 [==============================] - 0s 458us/sample - loss: 0.5305 - accuracy: 0.7645 - val_loss: 0.5867 - val_accuracy: 0.6557\n",
      "Epoch 16/50\n",
      "242/242 [==============================] - 0s 422us/sample - loss: 0.5089 - accuracy: 0.7727 - val_loss: 0.5800 - val_accuracy: 0.6721\n",
      "Epoch 17/50\n",
      "242/242 [==============================] - 0s 422us/sample - loss: 0.5126 - accuracy: 0.7686 - val_loss: 0.5847 - val_accuracy: 0.6721\n",
      "Epoch 18/50\n",
      "242/242 [==============================] - 0s 481us/sample - loss: 0.4831 - accuracy: 0.7851 - val_loss: 0.5714 - val_accuracy: 0.6885\n",
      "Epoch 19/50\n",
      "242/242 [==============================] - 0s 502us/sample - loss: 0.4995 - accuracy: 0.7810 - val_loss: 0.5706 - val_accuracy: 0.6885\n",
      "Epoch 20/50\n",
      "242/242 [==============================] - 0s 409us/sample - loss: 0.4889 - accuracy: 0.7851 - val_loss: 0.5594 - val_accuracy: 0.6885\n",
      "Epoch 21/50\n",
      "242/242 [==============================] - 0s 395us/sample - loss: 0.4603 - accuracy: 0.8223 - val_loss: 0.5552 - val_accuracy: 0.6885\n",
      "Epoch 22/50\n",
      "242/242 [==============================] - 0s 386us/sample - loss: 0.4897 - accuracy: 0.7645 - val_loss: 0.5491 - val_accuracy: 0.6885\n",
      "Epoch 23/50\n",
      "242/242 [==============================] - 0s 396us/sample - loss: 0.4457 - accuracy: 0.8099 - val_loss: 0.5521 - val_accuracy: 0.6885\n",
      "Epoch 24/50\n",
      "242/242 [==============================] - 0s 418us/sample - loss: 0.4534 - accuracy: 0.7975 - val_loss: 0.5534 - val_accuracy: 0.6885\n",
      "Epoch 25/50\n",
      "242/242 [==============================] - 0s 405us/sample - loss: 0.4687 - accuracy: 0.8182 - val_loss: 0.5423 - val_accuracy: 0.6885\n",
      "Epoch 26/50\n",
      "242/242 [==============================] - 0s 422us/sample - loss: 0.4592 - accuracy: 0.8058 - val_loss: 0.5517 - val_accuracy: 0.6885\n",
      "Epoch 27/50\n",
      "242/242 [==============================] - 0s 445us/sample - loss: 0.4455 - accuracy: 0.7810 - val_loss: 0.5478 - val_accuracy: 0.6885\n",
      "Epoch 28/50\n",
      "242/242 [==============================] - 0s 419us/sample - loss: 0.4363 - accuracy: 0.8058 - val_loss: 0.5413 - val_accuracy: 0.7049\n",
      "Epoch 29/50\n",
      "242/242 [==============================] - 0s 534us/sample - loss: 0.4474 - accuracy: 0.8058 - val_loss: 0.5396 - val_accuracy: 0.7049\n",
      "Epoch 30/50\n",
      "242/242 [==============================] - 0s 405us/sample - loss: 0.4247 - accuracy: 0.8182 - val_loss: 0.5450 - val_accuracy: 0.7377\n",
      "Epoch 31/50\n",
      "242/242 [==============================] - 0s 393us/sample - loss: 0.4257 - accuracy: 0.8347 - val_loss: 0.5470 - val_accuracy: 0.7541\n",
      "Epoch 32/50\n",
      "242/242 [==============================] - 0s 395us/sample - loss: 0.4298 - accuracy: 0.7934 - val_loss: 0.5329 - val_accuracy: 0.7213\n",
      "Epoch 33/50\n",
      "242/242 [==============================] - 0s 449us/sample - loss: 0.4377 - accuracy: 0.8140 - val_loss: 0.5286 - val_accuracy: 0.7213\n",
      "Epoch 34/50\n",
      "242/242 [==============================] - 0s 411us/sample - loss: 0.4189 - accuracy: 0.8264 - val_loss: 0.5377 - val_accuracy: 0.7213\n",
      "Epoch 35/50\n",
      "242/242 [==============================] - 0s 433us/sample - loss: 0.3869 - accuracy: 0.8595 - val_loss: 0.5307 - val_accuracy: 0.7213\n",
      "Epoch 36/50\n",
      "242/242 [==============================] - 0s 419us/sample - loss: 0.3976 - accuracy: 0.8388 - val_loss: 0.5377 - val_accuracy: 0.7541\n",
      "Epoch 37/50\n",
      "242/242 [==============================] - 0s 468us/sample - loss: 0.4092 - accuracy: 0.8430 - val_loss: 0.5359 - val_accuracy: 0.7541\n",
      "Epoch 38/50\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.4047 - accuracy: 0.8264 - val_loss: 0.5419 - val_accuracy: 0.7541\n",
      "Epoch 39/50\n",
      "242/242 [==============================] - 0s 417us/sample - loss: 0.4046 - accuracy: 0.8347 - val_loss: 0.5305 - val_accuracy: 0.7541\n",
      "Epoch 40/50\n",
      "242/242 [==============================] - 0s 441us/sample - loss: 0.4081 - accuracy: 0.8264 - val_loss: 0.5272 - val_accuracy: 0.7541\n",
      "Epoch 41/50\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.3938 - accuracy: 0.8306 - val_loss: 0.5323 - val_accuracy: 0.7541\n",
      "Epoch 42/50\n",
      "242/242 [==============================] - 0s 439us/sample - loss: 0.4125 - accuracy: 0.8264 - val_loss: 0.5305 - val_accuracy: 0.7541\n",
      "Epoch 43/50\n",
      "242/242 [==============================] - 0s 420us/sample - loss: 0.3766 - accuracy: 0.8595 - val_loss: 0.5287 - val_accuracy: 0.7541\n",
      "Epoch 44/50\n",
      "242/242 [==============================] - 0s 441us/sample - loss: 0.3580 - accuracy: 0.8388 - val_loss: 0.5322 - val_accuracy: 0.7541\n",
      "Epoch 45/50\n",
      "242/242 [==============================] - 0s 454us/sample - loss: 0.3937 - accuracy: 0.8388 - val_loss: 0.5272 - val_accuracy: 0.7541\n",
      "Epoch 46/50\n",
      "242/242 [==============================] - 0s 436us/sample - loss: 0.4067 - accuracy: 0.8264 - val_loss: 0.5228 - val_accuracy: 0.7541\n",
      "Epoch 47/50\n",
      "242/242 [==============================] - 0s 507us/sample - loss: 0.3808 - accuracy: 0.8306 - val_loss: 0.5254 - val_accuracy: 0.7705\n",
      "Epoch 48/50\n",
      "242/242 [==============================] - 0s 480us/sample - loss: 0.4137 - accuracy: 0.8306 - val_loss: 0.5284 - val_accuracy: 0.7705\n",
      "Epoch 49/50\n",
      "242/242 [==============================] - 0s 469us/sample - loss: 0.3968 - accuracy: 0.8430 - val_loss: 0.5271 - val_accuracy: 0.7705\n",
      "Epoch 50/50\n",
      "242/242 [==============================] - 0s 413us/sample - loss: 0.3538 - accuracy: 0.8430 - val_loss: 0.5289 - val_accuracy: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e4409dba8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test,y_test), \n",
    "          epochs=50, \n",
    "          batch_size=10, \n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BASE LINE ACCURACY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7704918032786885\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy_score(np.round(model.predict(X_test)),y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Grid Search to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def grid_create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(12, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model2 = KerasClassifier(build_fn=grid_create_model, verbose=0)\n",
    "\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
    "              'epochs': [10,20,30]\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model2, param_grid=param_grid)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.8347107364126474 using {'batch_size': 10, 'epochs': 30}\n",
      "Means: 0.710743782195178, Stdev: 0.06059231763158543 with: {'batch_size': 10, 'epochs': 10}\n",
      "Means: 0.8264462800065348, Stdev: 0.03558384325527004 with: {'batch_size': 10, 'epochs': 20}\n",
      "Means: 0.8347107364126474, Stdev: 0.03743872638495324 with: {'batch_size': 10, 'epochs': 30}\n",
      "Means: 0.5743801546983482, Stdev: 0.04857614697176276 with: {'batch_size': 20, 'epochs': 10}\n",
      "Means: 0.6611570223303865, Stdev: 0.08068327425907562 with: {'batch_size': 20, 'epochs': 20}\n",
      "Means: 0.7685950334407081, Stdev: 0.08371037981138875 with: {'batch_size': 20, 'epochs': 30}\n",
      "Means: 0.5991735682507192, Stdev: 0.04279934181207326 with: {'batch_size': 40, 'epochs': 10}\n",
      "Means: 0.6859504023859323, Stdev: 0.1149832127602226 with: {'batch_size': 40, 'epochs': 20}\n",
      "Means: 0.6157024840185464, Stdev: 0.04644640171040083 with: {'batch_size': 40, 'epochs': 30}\n",
      "Means: 0.5661157167647496, Stdev: 0.018916207976109507 with: {'batch_size': 60, 'epochs': 10}\n",
      "Means: 0.5702479457067064, Stdev: 0.017449612066027128 with: {'batch_size': 60, 'epochs': 20}\n",
      "Means: 0.7066115532532211, Stdev: 0.06614932671824107 with: {'batch_size': 60, 'epochs': 30}\n",
      "Means: 0.5330578576434742, Stdev: 0.060301649857494026 with: {'batch_size': 80, 'epochs': 10}\n",
      "Means: 0.6239669419024602, Stdev: 0.10913307573981781 with: {'batch_size': 80, 'epochs': 20}\n",
      "Means: 0.574380171693061, Stdev: 0.015157765417821715 with: {'batch_size': 80, 'epochs': 30}\n",
      "Means: 0.5041322427347672, Stdev: 0.06864426776864169 with: {'batch_size': 100, 'epochs': 10}\n",
      "Means: 0.5289256263616656, Stdev: 0.06238924553756254 with: {'batch_size': 100, 'epochs': 20}\n",
      "Means: 0.5826446248972712, Stdev: 0.09334987421253058 with: {'batch_size': 100, 'epochs': 30}\n"
     ]
    }
   ],
   "source": [
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Parameters:\n",
    "#### batch_size: 10\n",
    "#### epochs: 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples\n",
      "Epoch 1/30\n",
      "242/242 [==============================] - 1s 2ms/sample - loss: 0.7061 - accuracy: 0.4339\n",
      "Epoch 2/30\n",
      "242/242 [==============================] - 0s 390us/sample - loss: 0.6897 - accuracy: 0.5124\n",
      "Epoch 3/30\n",
      "242/242 [==============================] - 0s 375us/sample - loss: 0.6739 - accuracy: 0.7314\n",
      "Epoch 4/30\n",
      "242/242 [==============================] - 0s 308us/sample - loss: 0.6557 - accuracy: 0.7025\n",
      "Epoch 5/30\n",
      "242/242 [==============================] - 0s 308us/sample - loss: 0.6345 - accuracy: 0.7397\n",
      "Epoch 6/30\n",
      "242/242 [==============================] - 0s 337us/sample - loss: 0.6120 - accuracy: 0.7314\n",
      "Epoch 7/30\n",
      "242/242 [==============================] - 0s 331us/sample - loss: 0.5862 - accuracy: 0.8099\n",
      "Epoch 8/30\n",
      "242/242 [==============================] - 0s 296us/sample - loss: 0.5610 - accuracy: 0.8182\n",
      "Epoch 9/30\n",
      "242/242 [==============================] - 0s 300us/sample - loss: 0.5374 - accuracy: 0.8099\n",
      "Epoch 10/30\n",
      "242/242 [==============================] - 0s 293us/sample - loss: 0.5144 - accuracy: 0.8223\n",
      "Epoch 11/30\n",
      "242/242 [==============================] - 0s 302us/sample - loss: 0.4937 - accuracy: 0.8388\n",
      "Epoch 12/30\n",
      "242/242 [==============================] - 0s 328us/sample - loss: 0.4758 - accuracy: 0.8223\n",
      "Epoch 13/30\n",
      "242/242 [==============================] - 0s 363us/sample - loss: 0.4584 - accuracy: 0.8388\n",
      "Epoch 14/30\n",
      "242/242 [==============================] - 0s 323us/sample - loss: 0.4446 - accuracy: 0.8347\n",
      "Epoch 15/30\n",
      "242/242 [==============================] - 0s 328us/sample - loss: 0.4327 - accuracy: 0.8430\n",
      "Epoch 16/30\n",
      "242/242 [==============================] - 0s 351us/sample - loss: 0.4225 - accuracy: 0.8388\n",
      "Epoch 17/30\n",
      "242/242 [==============================] - 0s 363us/sample - loss: 0.4130 - accuracy: 0.8264\n",
      "Epoch 18/30\n",
      "242/242 [==============================] - 0s 323us/sample - loss: 0.4096 - accuracy: 0.8182\n",
      "Epoch 19/30\n",
      "242/242 [==============================] - 0s 317us/sample - loss: 0.3995 - accuracy: 0.8264\n",
      "Epoch 20/30\n",
      "242/242 [==============================] - 0s 295us/sample - loss: 0.3935 - accuracy: 0.8388\n",
      "Epoch 21/30\n",
      "242/242 [==============================] - 0s 298us/sample - loss: 0.3882 - accuracy: 0.8347\n",
      "Epoch 22/30\n",
      "242/242 [==============================] - 0s 308us/sample - loss: 0.3835 - accuracy: 0.8306\n",
      "Epoch 23/30\n",
      "242/242 [==============================] - 0s 321us/sample - loss: 0.3808 - accuracy: 0.8471\n",
      "Epoch 24/30\n",
      "242/242 [==============================] - 0s 311us/sample - loss: 0.3751 - accuracy: 0.8512\n",
      "Epoch 25/30\n",
      "242/242 [==============================] - 0s 284us/sample - loss: 0.3717 - accuracy: 0.8430\n",
      "Epoch 26/30\n",
      "242/242 [==============================] - 0s 291us/sample - loss: 0.3683 - accuracy: 0.8512\n",
      "Epoch 27/30\n",
      "242/242 [==============================] - 0s 295us/sample - loss: 0.3649 - accuracy: 0.8512\n",
      "Epoch 28/30\n",
      "242/242 [==============================] - 0s 282us/sample - loss: 0.3618 - accuracy: 0.8554\n",
      "Epoch 29/30\n",
      "242/242 [==============================] - 0s 339us/sample - loss: 0.3586 - accuracy: 0.8595\n",
      "Epoch 30/30\n",
      "242/242 [==============================] - 0s 291us/sample - loss: 0.3575 - accuracy: 0.8636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5d257c5b00>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = KerasClassifier(build_fn=grid_create_model, verbose=1, batch_size= 10,epochs= 30)\n",
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We achieve an accuracy of: 0.8636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
